[{"id":0,"href":"/notebook/docs/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/Node/","title":"Node","section":"安装部署","content":"目录 #  "},{"id":1,"href":"/notebook/docs/%E9%97%AE%E9%A2%98%E9%9B%86/%E5%BE%85%E7%A1%AE%E8%AE%A4/","title":"待确认","section":"问题集","content":"目录 #  "},{"id":2,"href":"/notebook/docs/k8s/dashboard/dashborad%E5%AE%89%E8%A3%85/","title":"Dashborad安装","section":"Dashboard","content":"NodePort部署 #  wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.2.0/aio/deploy/recommended.yaml vim recommended.yaml kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: type: NodePort //添加类型为NodePort ports: - port: 443 targetPort: 8443 nodePort: 31010 //映射到宿主机的端口为31010 selector: k8s-app: kubernetes-dashboard //执行yaml文件 $ kubectl apply -f recommended.yaml //查看Pod是否运行 $ kubectl get pod -n kubernetes-dashboard //确保该yaml文件提供的pod都正常运行 $ kubectl get svc -n kubernetes-dashboard 访问 #  创建dashboard的管理用户\n$ kubectl create serviceaccount dashboard-admin -n kube-system 绑定用户为集群的管理员\n$ kubectl create clusterrolebinding dashboard-cluster-rolebinding --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin 获取刚刚创建的用户Token\nkubectl get secrets -n kube-system | grep dashboard-admin kubectl describe secrets -n kube-system dashboard-admin-token-xxx eyJhbGciOiJSUzI1NiIsImtpZCI6InVWaHV5OVR4Zkl5bUFtWm1VSV9uWDR1aEhfSkdQQkFadjZPVi0wOVgyVGcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tZmo5dG0iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiOTdlNzc2ZDgtMTY3NS00ZjIxLTljMDItNTYxNWM3M2UzZTBlIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.WySaF3V1tHckuK2iF6FNO-ZQeqr_8A-4hdB9DuWFE0apdXOAe1iql1YApOmGRgq4KtbLHEtcLD5hX2_xMb-Qz_gYHcFWdZn-XggAI45TWmSznwMzqQxxlTMnJIV2j7FSwynMOLX2kL6INHLamuT4VjX_R3bVGCrL9rapPLtzRXnf1_UH4ztv7F6N2ye8ALkgotmL-EQCTe_iexHg2DfHpfsjV7oLoByfGcZdFdHr-xNnoowfj-TbF82iX1MEgnFSChkvdaArIZhpQqFIwc_P8CKTcZd4LxejSdlG-_V3BGeyUOP0BIxw2M8rAXeCxljk-bdgECoOYQyQqksfu4EZiA "},{"id":3,"href":"/notebook/docs/k8s/kubectl/kubectl%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"Kubectl常用命令","section":"Kubectl","content":"自动补全 #  source \u0026lt;(kubectl completion bash) 常用举例 #  生成pod\nkubectl run pod --image=nginx --restart=Never --requests= 生成job\nkubectl run job --image=nginx --restart=OnFailure 生成deploy\nkubectl run deploy --image=nginx --replicas=2 生成svc\nkubectl expose deployment sd2 --type=ClusterIP --target-port=80 --name=sd2 生成名称空间\nkubectl create ns ns 生成configmap\nkubectl create configmap db2 --from-literal=nt=nt 查看 #  查看所有namespace的pods运行情况\nkubectl get pods --all-namespaces 查看具体pods，记得后边跟namespace名字哦\nkubectl get pods kubernetes-dashboard-76479d66bb-nj8wr --namespace=kube-system 查看pods具体信息\nkubectl get pods -o wide kubernetes-dashboard-76479d66bb-nj8wr --namespace=kube-system 查看集群健康状态\nkubectl get cs 获取所有deployment\nkubectl get deployment --all-namespaces 列出该 namespace 中的所有 pod 包括未初始化的\nkubectl get pods --include-uninitialized 查看deployment()\nkubectl get deployment nginx-app 查看deployment和servers\nkubectl get deploy,services 查看pod详情\nkubectl describe pods xxxxpodsname --namespace=xxxnamespace 查看pod日志\nkubectl logs $POD_NAME 查看pod变量\nkubectl exec my-nginx-5j8ok -- printenv | grep SERVICE 集群 #  集群健康情况\nkubectl get cs 集群核心组件运行情况\nkubectl cluster-info 命名空间\nkubectl get namespaces 版本\nkubectl version api\nkubectl api-versions 查看事件\nkubectl get events 获取全部节点\nkubectl get nodes 删除节点\nkubectl delete node k8s2 创建 #  创建资源\nkubectl create -f ./nginx.yaml 创建当前目录下的所有yaml资源\nkubectl create -f ./ 使用多个文件创建资源\nkubectl create -f ./nginx1.yaml -f ./mysql2.yaml 使用目录下的所有文件夹的yaml来创建资源\nkubectl create -f ./dir 使用 url 来创建资源\nkubectl create -f https://xxxx/xxx 创建带有终端的pod\nkubectl run -i --tty busybox --image=busybox 创建一个nginx实例\nkubectl run nginx --image=nginx 创建deployment\nkubectl run mybusybox --image=busybox --replicas=5 获取 pod 和 svc 的api文档\nkubectl explain pods,svc 更新 #  滚动更新 pod frontend-v1\nkubectl rolling-update python-v1 -f python-v2.json 更新资源名称并更新镜像\nkubectl rolling-update python-v1 python-v2 --image=image:v2 更新 frontend pod 中的镜像\nkubectl rolling-update python --image=image:v2 退出已存在的进行中的滚动更新\nkubectl rolling-update python-v1 python-v2 --rollback 为 nginx RC 创建svc\nkubectl expose rc nginx --port=80 --target-port=8000 添加标签\nkubectl label pods nginx-pod new-label=awesome 添加注解\nkubectl annotate pods nginx-pod icon-url=http://goo.gl/XXBTWq 编辑资源 #  编辑名为 docker-registry 的 service\nkubectl edit svc/docker-registry 动态伸缩pod #  HPA\nkubectl autoscale deployment foo --min=2 --max=10 将foo副本集变成3个\nkubectl scale --replicas=3 rs/foo 变更“foo”中指定的资源数量\nkubectl scale --replicas=3 -f foo.yaml 将deployment/mysql从2个变成3个\nkubectl scale --current-replicas=2 --replicas=3 deployment/mysql 变更多个控制器的数量\nkubectl scale --replicas=5 rc/foo rc/bar rc/baz 查看变更进度\nkubectl rollout status deploy deployment/mysql 删除 #  删除 pod.json 文件中定义的类型和名称的 pod\nkubectl delete -f ./pod.json 删除名为“baz”的 pod 和名为“foo”的 service\nkubectl delete pod,service baz foo 删除具有 name=myLabel 标签的 pod 和 serivce\nkubectl delete pods,services -l name=myLabel 删除具有 name=myLabel 标签的 pod 和 service，包括尚未初始化的\nkubectl delete pods,services -l name=myLabel --include-uninitialized 删除 my-ns namespace下的所有 pod 和 serivce，包括尚未初始化的\nkubectl -n my-ns delete po,svc --all 强制删除\nkubectl delete pods prometheus-7fcfcb9f89-qkkf7 --grace-period=0 --force 交互 #  输出pod的日志\nkubectl logs nginx-pod 输出pod中容器的日志\nkubectl logs nginx-pod -c my-container 滚动持续输出pod的日志\nkubectl logs -f nginx-pod 滚动持续输出pod中容器的日志\nkubectl logs -f nginx-pod -c my-container 交互式 shell 的方式运行 pod\nkubectl run -i --tty busybox --image=busybox -- sh 连接到运行中的容器\nkubectl attach nginx-pod -i 在已存在的pod单容器中执行命令\nkubectl exec nginx-pod -- ls / 在已存在的pod中的容器执行命令\nkubectl exec nginx-pod -c my-container -- ls / metrics 显示指定pod和容器的监控指标度量\nkubectl top pod POD_NAME --containers 调度配置 #  标记my-node不可调度\nkubectl cordon k8s-node 驱逐my-node上的pod以待维护\nkubectl drain k8s-node 标记my-node可调度\nkubectl uncordon k8s-node metrics 显示my-node的监控指标度量\nkubectl top node k8s-node 将当前集群状态输出到 stdout\nkubectl cluster-info dump 将当前集群状态输出到 /path/to/cluster-state\nkubectl cluster-info dump --output-directory=/path/to/cluster-state 如果该键和影响的污点（taint）已存在，则使用指定的值替换\nkubectl taint nodes foo dedicated=special-user:NoSchedule "},{"id":4,"href":"/notebook/docs/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/Node/Mac%E5%AE%89%E8%A3%85NVM/","title":"Mac安装 Nvm","section":"Node","content":"1.卸载Node.js #  1.1 通过Homebrew安装 #  brew uninstall node 1.2 通过官方安装包安装 #   删除 /usr/local/lib 下的任意 node 和 node_modules 的文件或目录 删除 /usr/local/include 下的任意 node 和 node_modules 的文件或目录 删除 Home 目录下的任意 node 和 node_modules 的文件或目录 删除 /usr/local/bin 下的任意 node 的可执行文件  以下三种任选其一。测试 nvm、node、npm 三个命令是否还在\nsudo rm -rf /opt/local/bin/node /opt/local/include/node /opt/local/lib/node_modules sudo rm -rf /usr/local/bin/npm /usr/local/share/man/man1/node.1 /usr/local/lib/dtrace/node.d sudo rm -rf /usr/local/{bin/{node,npm},lib/node_modules/npm,lib/node,share/man/*/node.*} sudo rm -rf ~/.npm sudo rm -rf ~/node_modules sudo rm -rf ~/.node-gyp sudo rm /usr/local/bin/node sudo rm /usr/local/bin/npm sudo rm /usr/local/lib/dtrace/node.d 2. 安装版本管理Nvm #  brew install nvm 安装完成后，在 ~/.bash_profile 或 ~/.zshrc 中添加以下内容\n# Node.js source $(brew --prefix nvm)/nvm.sh 安装完成后，查看是否安装成功\n$ nvm --version 0.39.1 $ nvm list 3. 通过 nvm 安装管理Node.js #  安装最新版本node.js\nnvm install node 安装指定版本node.js\nnvm install 10.22.0 列出所有版本\n$ nvm list -\u0026gt; v10.22.0 v17.4.0 default -\u0026gt; node (-\u0026gt; v17.4.0) iojs -\u0026gt; N/A (default) unstable -\u0026gt; N/A (default) node -\u0026gt; stable (-\u0026gt; v17.4.0) (default) stable -\u0026gt; 17.4 (-\u0026gt; v17.4.0) (default) lts/* -\u0026gt; lts/gallium (-\u0026gt; N/A) lts/argon -\u0026gt; v4.9.1 (-\u0026gt; N/A) lts/boron -\u0026gt; v6.17.1 (-\u0026gt; N/A) lts/carbon -\u0026gt; v8.17.0 (-\u0026gt; N/A) lts/dubnium -\u0026gt; v10.24.1 (-\u0026gt; N/A) lts/erbium -\u0026gt; v12.22.9 (-\u0026gt; N/A) lts/fermium -\u0026gt; v14.18.3 (-\u0026gt; N/A) lts/gallium -\u0026gt; v16.13.2 (-\u0026gt; N/A) 显示当前版本\n$ nvm current v10.22.0 切换使用指定的版本node\nnvm use 17.4.0 4. 安装npm包 #  nvm use 10.22.0 npm install -g gitbook-cli "},{"id":5,"href":"/notebook/docs/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/openyurt/","title":"Openyurt","section":"边缘计算","content":"服务类型权限\u0026mdash;和机器标签绑定\napi权限\n权限最小原则\n"},{"id":6,"href":"/notebook/docs/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/WasmEdge/","title":"Wasm Edge","section":"边缘计算","content":"服务类型权限\u0026mdash;和机器标签绑定\napi权限\n权限最小原则\n"},{"id":7,"href":"/notebook/docs/%E9%97%AE%E9%A2%98%E9%9B%86/%E5%B7%B2%E8%A7%A3%E5%86%B3/issue/","title":"Issue","section":"已解决","content":"问题1：Metrics Server部署失败 #  Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 14m default-scheduler Successfully assigned kube-system/metrics-server-7898cc74cb-jdwps to master Normal Pulling 14m kubelet Pulling image \u0026#34;k8s.gcr.io/metrics-server/metrics-server:v0.5.2\u0026#34; Normal Pulled 13m kubelet Successfully pulled image \u0026#34;k8s.gcr.io/metrics-server/metrics-server:v0.5.2\u0026#34; in 16.557781304s Normal Created 13m kubelet Created container metrics-server Normal Started 13m kubelet Started container metrics-server Warning Unhealthy 3m54s (x57 over 13m) kubelet Readiness probe failed: HTTP probe failed with statuscode: 500 修改components.yaml, 添加--kubelet-insecure-tls\napiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-system spec: selector: matchLabels: k8s-app: metrics-server strategy: rollingUpdate: maxUnavailable: 0 template: metadata: labels: k8s-app: metrics-server spec: containers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls "},{"id":8,"href":"/notebook/docs/%E9%97%AE%E9%A2%98%E9%9B%86/%E5%BE%85%E7%A1%AE%E8%AE%A4/ISSUE/","title":"Issue","section":"待确认","content":"yurtctl convert错误 #  [root@master ~]# yurtctl convert --provider kubeadm --kubeadm-conf-path /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf --cloud-nodes master I1201 02:01:31.538410 42272 convert.go:318] mark master as the cloud-node I1201 02:02:51.677158 42272 util.go:543] servant job(yurtctl-disable-node-controller-master) has succeeded I1201 02:02:51.677200 42272 convert.go:343] complete disabling node-controller I1201 02:02:51.678398 42272 convert.go:443] kube-public/cluster-info configmap already exists, skip to prepare it I1201 02:02:51.690122 42272 convert.go:408] deploying the yurt-hub and resetting the kubelet service on edge nodes... E1201 02:04:51.694906 42272 util.go:540] fail to run servant job(yurtctl-servant-convert-node1): wait for job to be complete timeout I1201 02:04:51.694954 42272 convert.go:414] complete deploying yurt-hub on edge nodes I1201 02:04:51.694963 42272 convert.go:417] deploying the yurt-hub and resetting the kubelet service on cloud nodes E1201 02:06:51.700340 42272 util.go:540] fail to run servant job(yurtctl-servant-convert-master): wait for job to be complete timeout I1201 02:06:51.700441 42272 convert.go:423] complete deploying yurt-hub on cloud nodes [root@master ~]# kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system cilium-89zdb 1/1 Running 17 23d kube-system cilium-bt9zk 1/1 Running 41 23d kube-system cilium-operator-57b67c8d5f-rldrh 1/1 Running 1 21h kube-system cilium-operator-57b67c8d5f-tdp8m 1/1 Running 123 23d kube-system coredns-7f89b7bc75-t6vq7 1/1 Running 1 21h kube-system coredns-7f89b7bc75-v259v 1/1 Running 1 21h kube-system etcd-master 1/1 Running 1 21h kube-system kube-apiserver-master 1/1 Running 1 21h kube-system kube-controller-manager-master 1/1 Running 0 10m kube-system kube-scheduler-master 1/1 Running 1 21h kube-system yurt-controller-manager-77b97fd47b-m2n6l 1/1 Running 0 11m kube-system yurt-hub-master 1/1 Running 0 8m25s kube-system yurt-hub-node1 1/1 Running 0 9m53s kube-system yurtctl-servant-convert-master-tz8qm 0/1 Completed 1 8m26s kube-system yurtctl-servant-convert-node1-mtrgk 0/1 Completed 2 10m [root@master ~]# yurtctl revert --kubeadm-conf-path /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf I1201 02:14:33.097932 44924 revert.go:172] yurt controller manager is removed I1201 02:14:33.100463 44924 revert.go:182] serviceaccount for yurt controller manager is removed I1201 02:14:33.102799 44924 revert.go:192] clusterrole for yurt controller manager is removed I1201 02:14:33.106710 44924 revert.go:202] clusterrolebinding for yurt controller manager is removed I1201 02:14:33.123802 44924 revert.go:353] deployment for yurt app manager is removed I1201 02:14:33.124798 44924 revert.go:363] Role for yurt app manager is removed I1201 02:14:33.125830 44924 revert.go:372] ClusterRole for yurt app manager is removed I1201 02:14:33.129656 44924 revert.go:381] ClusterRoleBinding for yurt app manager is removed I1201 02:14:33.130883 44924 revert.go:391] RoleBinding for yurt app manager is removed I1201 02:14:33.275224 44924 revert.go:401] secret for yurt app manager is removed I1201 02:14:33.474971 44924 revert.go:411] Service for yurt app manager is removed I1201 02:14:33.476301 44924 revert.go:421] MutatingWebhookConfiguration for yurt app manager is removed I1201 02:14:33.477440 44924 revert.go:431] ValidatingWebhookConfiguration for yurt app manager is removed I1201 02:14:33.492314 44924 revert.go:440] crd for yurt app manager is removed I1201 02:14:33.502202 44924 revert.go:449] UnitedDeploymentcrd for yurt app manager is removed I1201 02:14:33.502231 44924 revert.go:221] yurt app manager is removed I1201 02:15:23.523798 44924 util.go:543] servant job(yurtctl-enable-node-controller-master) has succeeded I1201 02:15:23.523830 44924 revert.go:234] complete enabling node-controller I1201 02:15:43.535988 44924 util.go:543] servant job(yurtctl-servant-revert-node1) has succeeded I1201 02:15:43.536038 44924 revert.go:247] complete removing yurt-hub and resetting kubelet service on edge nodes I1201 02:15:53.544282 44924 util.go:543] servant job(yurtctl-servant-revert-master) has succeeded I1201 02:15:53.544320 44924 revert.go:255] complete removing yurt-hub and resetting kubelet service on cloud nodes I1201 02:15:53.551972 44924 revert.go:263] delete yurthub clusterrole and clusterrolebinding [root@master ~]# kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE awx awx-demo-7df55fb78c-hr9mz 4/4 Running 24 6d23h awx awx-demo-postgres-0 1/1 Running 6 6d23h awx awx-operator-controller-manager-877886796-ljlvv 2/2 Running 28 7d default details-v1-66b6955995-67fhq 2/2 Running 80 23d default httpbin-66cdbdb6c5-zqrd7 2/2 Running 18 23d default productpage-v1-5d9b4c9849-nbjzp 2/2 Running 79 23d default ratings-v1-fd78f799f-4rxhw 2/2 Running 78 23d default reviews-v1-6549ddccc5-fctws 2/2 Running 79 23d default reviews-v2-76c4865449-jdzr2 2/2 Running 78 23d default reviews-v3-6b554c875-l7rj5 2/2 Running 77 23d istio-system istio-egressgateway-797b55644-r8v47 1/1 Running 40 23d istio-system istio-ingressgateway-5ff88fbcc7-9bbdx 1/1 Running 40 23d istio-system istiod-8fd46755b-dww2b 1/1 Running 40 23d kube-system cilium-89zdb 1/1 Running 17 23d kube-system cilium-bt9zk 1/1 Running 41 23d kube-system cilium-operator-57b67c8d5f-rldrh 1/1 Running 1 21h kube-system cilium-operator-57b67c8d5f-tdp8m 1/1 Running 123 23d kube-system coredns-7f89b7bc75-t6vq7 1/1 Running 1 21h kube-system coredns-7f89b7bc75-v259v 1/1 Running 1 21h kube-system etcd-master 1/1 Running 1 21h kube-system hubble-relay-6fcf45468f-pls4w 1/1 Running 277 23d kube-system hubble-ui-7cd68fdf6d-c25b7 3/3 Running 119 23d kube-system kube-apiserver-master 1/1 Running 1 21h kube-system kube-controller-manager-master 1/1 Running 0 2m20s kube-system kube-scheduler-master 0/1 Running 1 21h kube-system yurtctl-servant-convert-master-tz8qm 0/1 Completed 1 12m kube-system yurtctl-servant-convert-node1-mtrgk 0/1 Completed 2 14m [root@master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready control-plane,master 23d v1.20.13 node1 Ready \u0026lt;none\u0026gt; 23d v1.20.13 VCluster #  [root@master ~]# kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/cluster-api-provider-nested/master/virtualcluster/config/sampleswithspec/clusterversion_v1_nodeport.yaml clusterversion.tenancy.x-k8s.io/cv-sample-np created [root@master ~]# kubectl vc create -f https://raw.githubusercontent.com/kubernetes-sigs/cluster-api-provider-nested/master/virtualcluster/config/sampleswithspec/virtualcluster_1_nodeport.yaml -o vc-1.kubeconfig 2021/12/05 23:56:04 etcd is ready cannot find sts/apiserver in ns default-56325d-vc-sample-1: default-56325d-vc-sample-1/apiserver is not ready in 120 seconds [root@master ~]# kubectl get pod -ndefault-56325d-vc-sample-1 NAME READY STATUS RESTARTS AGE apiserver-0 0/1 ContainerCreating 0 3m16s etcd-0 1/1 Running 0 4m6s [root@master ~]# kubectl describe pod -ndefault-56325d-vc-sample-1 apiserver-0 Name: apiserver-0 Namespace: default-56325d-vc-sample-1 Priority: 0 Node: node1/192.168.92.135 Start Time: Sun, 05 Dec 2021 23:56:03 -0500 Labels: component-name=apiserver controller-revision-hash=apiserver-77b4b4dfbf statefulset.kubernetes.io/pod-name=apiserver-0 Annotations: \u0026lt;none\u0026gt; Status: Pending IP: IPs: \u0026lt;none\u0026gt; Controlled By: StatefulSet/apiserver Containers: apiserver: Container ID: Image: virtualcluster/apiserver-v1.16.2 Image ID: Port: 6443/TCP Host Port: 0/TCP Command: kube-apiserver Args: --bind-address=0.0.0.0 --allow-privileged=true --anonymous-auth=true --client-ca-file=/etc/kubernetes/pki/root/tls.crt --tls-cert-file=/etc/kubernetes/pki/apiserver/tls.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver/tls.key --kubelet-https=true --kubelet-client-certificate=/etc/kubernetes/pki/apiserver/tls.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver/tls.key --enable-bootstrap-token-auth=true --etcd-servers=https://etcd-0.etcd:2379 --etcd-cafile=/etc/kubernetes/pki/root/tls.crt --etcd-certfile=/etc/kubernetes/pki/apiserver/tls.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver/tls.key --service-account-key-file=/etc/kubernetes/pki/service-account/tls.key --service-cluster-ip-range=10.32.0.0/16 --service-node-port-range=30000-32767 --authorization-mode=Node,RBAC --runtime-config=api/all --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota --apiserver-count=1 --endpoint-reconciler-type=master-count --enable-aggregator-routing=true --requestheader-client-ca-file=/etc/kubernetes/pki/root/tls.crt --requestheader-allowed-names=front-proxy-client --requestheader-username-headers=X-Remote-User --requestheader-group-headers=X-Remote-Group --requestheader-extra-headers-prefix=X-Remote-Extra- --proxy-client-key-file=/etc/kubernetes/pki/frontproxy/tls.key --proxy-client-cert-file=/etc/kubernetes/pki/frontproxy/tls.crt --v=2 State: Waiting Reason: ContainerCreating Ready: False Restart Count: 0 Liveness: tcp-socket :6443 delay=15s timeout=15s period=10s #success=1 #failure=8 Readiness: http-get https://:6443/healthz delay=5s timeout=30s period=2s #success=1 #failure=8 Environment: \u0026lt;none\u0026gt; Mounts: /etc/kubernetes/pki/apiserver from apiserver-ca (ro) /etc/kubernetes/pki/frontproxy from front-proxy-ca (ro) /etc/kubernetes/pki/root from root-ca (ro) /etc/kubernetes/pki/service-account from serviceaccount-rsa (ro) /var/run/secrets/kubernetes.io/serviceaccount from default-token-8xcwl (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: apiserver-ca: Type: Secret (a volume populated by a Secret) SecretName: apiserver-ca Optional: false root-ca: Type: Secret (a volume populated by a Secret) SecretName: root-ca Optional: false front-proxy-ca: Type: Secret (a volume populated by a Secret) SecretName: front-proxy-ca Optional: false serviceaccount-rsa: Type: Secret (a volume populated by a Secret) SecretName: serviceaccount-rsa Optional: false default-token-8xcwl: Type: Secret (a volume populated by a Secret) SecretName: default-token-8xcwl Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m29s default-scheduler Successfully assigned default-56325d-vc-sample-1/apiserver-0 to node1 Warning FailedMount 86s kubelet Unable to attach or mount volumes: unmounted volumes=[front-proxy-ca], unattached volumes=[default-token-8xcwl apiserver-ca front-proxy-ca root-ca serviceaccount-rsa]: timed out waiting for the condition Warning FailedMount 81s (x9 over 3m29s) kubelet MountVolume.SetUp failed for volume \u0026quot;front-proxy-ca\u0026quot; : secret \u0026quot;front-proxy-ca\u0026quot; not found [root@master ~]# kubectl get all -n vc-manager NAME READY STATUS RESTARTS AGE pod/vc-manager-76c5878465-25s5m 1/1 Running 0 67m pod/vc-syncer-55c5bc5898-47vzz 1/1 Running 0 67m pod/vn-agent-t7wg9 1/1 Running 0 67m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/virtualcluster-webhook-service ClusterIP 10.10.180.101 \u0026lt;none\u0026gt; 9443/TCP 65m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/vn-agent 1 1 1 1 1 \u0026lt;none\u0026gt; 67m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/vc-manager 1/1 1 1 67m deployment.apps/vc-syncer 1/1 1 1 67m NAME DESIRED CURRENT READY AGE replicaset.apps/vc-manager-76c5878465 1 1 1 67m replicaset.apps/vc-syncer-55c5bc5898 1 1 1 67m Karmada #  [root@alicloud-node karmada]# hack/local-up-karmada.sh kind not exists, will install kind v0.11.1 Installing \u0026#39;kind v0.11.1\u0026#39; for you which: no kind in (/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/lib/golang/bin:/root/bin) Installing \u0026#39;kubectl v1.18.0\u0026#39; for you Preparing kindClusterConfig in path: /tmp/tmp.nKvTwSYbZE Creating cluster karmada-host Creating cluster member1 Creating cluster member2 Creating cluster member3 make: Entering directory \u0026#39;/root/Develop/gopath/src/karmada\u0026#39; CGO_ENABLED=0 GOOS=linux go build \\ \t-ldflags \u0026#34;-X github.com/karmada-io/karmada/pkg/version.gitVersion=v0.9.0-115-g1f87ca95 -X github.com/karmada-io/karmada/pkg/version.gitCommit=1f87ca9519957e76fccb660079bfc44d58a09286 -X github.com/karmada-io/karmada/pkg/version.gitTreeState=\u0026#34;clean\u0026#34; -X github.com/karmada-io/karmada/pkg/version.buildDate=2021-12-24T07:38:15Z\u0026#34; \\ \t-o karmada-controller-manager \\ \tcmd/controller-manager/controller-manager.go VERSION=latest hack/docker.sh karmada-controller-manager Sending build context to Docker daemon 160.8MB Step 1/4 : FROM alpine:3.7 3.7: Pulling from library/alpine 5d20c808ce19: Pull complete Digest: sha256:8421d9a84432575381bfabd248f1eb56f3aa21d9d7cd2511583c68c9b7511d10 Status: Downloaded newer image for alpine:3.7 ---\u0026gt; 6d1ef012b567 Step 2/4 : RUN apk add --no-cache ca-certificates ---\u0026gt; Running in 21581f969cf1 fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz WARNING: Ignoring http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz: operation timed out fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz ERROR: unsatisfiable constraints: ca-certificates (missing): required by: world[ca-certificates] The command \u0026#39;/bin/sh -c apk add --no-cache ca-certificates\u0026#39; returned a non-zero code: 1 make: *** [Makefile:104: image-karmada-controller-manager] Error 1 make: Leaving directory \u0026#39;/root/Develop/gopath/src/karmada\u0026#39; [root@alicloud-node karmada]# hack/local-up-karmada.sh kind not exists, will install kind v0.11.1 Installing \u0026#39;kind v0.11.1\u0026#39; for you which: no kind in (/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/lib/golang/bin:/root/bin) Installing \u0026#39;kubectl v1.18.0\u0026#39; for you Preparing kindClusterConfig in path: /tmp/tmp.axBYDaONXq Creating cluster karmada-host Creating cluster member1 Creating cluster member2 Creating cluster member3 make: Entering directory \u0026#39;/root/Develop/gopath/src/karmada\u0026#39; VERSION=latest hack/docker.sh karmada-controller-manager Sending build context to Docker daemon 399.8MB Step 1/4 : FROM alpine:3.7 ---\u0026gt; 6d1ef012b567 Step 2/4 : RUN apk add --no-cache ca-certificates ---\u0026gt; Using cache ---\u0026gt; 987b2c96f3e3 Step 3/4 : ADD karmada-controller-manager /bin/ ---\u0026gt; Using cache ---\u0026gt; 4c8132a35cf3 Step 4/4 : CMD [\u0026#34;/bin/karmada-controller-manager\u0026#34;] ---\u0026gt; Using cache ---\u0026gt; 50d6149c965b Successfully built 50d6149c965b Successfully tagged swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-controller-manager:latest VERSION=latest hack/docker.sh karmada-scheduler Sending build context to Docker daemon 399.8MB Step 1/4 : FROM alpine:3.7 ---\u0026gt; 6d1ef012b567 Step 2/4 : RUN apk add --no-cache ca-certificates ---\u0026gt; Using cache ---\u0026gt; 987b2c96f3e3 Step 3/4 : ADD karmada-scheduler /bin/ ---\u0026gt; Using cache ---\u0026gt; fe3334c67cb4 Step 4/4 : CMD [\u0026#34;/bin/karmada-scheduler\u0026#34;] ---\u0026gt; Using cache ---\u0026gt; 47c9db1b4870 Successfully built 47c9db1b4870 Successfully tagged swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-scheduler:latest VERSION=latest hack/docker.sh karmada-webhook Sending build context to Docker daemon 399.8MB Step 1/4 : FROM alpine:3.7 ---\u0026gt; 6d1ef012b567 Step 2/4 : RUN apk add --no-cache ca-certificates ---\u0026gt; Using cache ---\u0026gt; 987b2c96f3e3 Step 3/4 : ADD karmada-webhook /bin/ ---\u0026gt; Using cache ---\u0026gt; 836cbce55275 Step 4/4 : CMD [\u0026#34;/bin/karmada-webhook\u0026#34;] ---\u0026gt; Using cache ---\u0026gt; baa6f4c411c2 Successfully built baa6f4c411c2 Successfully tagged swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-webhook:latest VERSION=latest hack/docker.sh karmada-agent Sending build context to Docker daemon 399.8MB Step 1/4 : FROM alpine:3.7 ---\u0026gt; 6d1ef012b567 Step 2/4 : RUN apk add --no-cache ca-certificates ---\u0026gt; Using cache ---\u0026gt; 987b2c96f3e3 Step 3/4 : ADD karmada-agent /bin/ ---\u0026gt; Using cache ---\u0026gt; 857b355f9fab Step 4/4 : CMD [\u0026#34;/bin/karmada-agent\u0026#34;] ---\u0026gt; Using cache ---\u0026gt; 1686da0562fc Successfully built 1686da0562fc Successfully tagged swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-agent:latest VERSION=latest hack/docker.sh karmada-scheduler-estimator Sending build context to Docker daemon 399.8MB Step 1/4 : FROM alpine:3.7 ---\u0026gt; 6d1ef012b567 Step 2/4 : RUN apk add --no-cache ca-certificates ---\u0026gt; Using cache ---\u0026gt; 987b2c96f3e3 Step 3/4 : ADD karmada-scheduler-estimator /bin/ ---\u0026gt; Using cache ---\u0026gt; aea9856d3e77 Step 4/4 : CMD [\u0026#34;/bin/karmada-scheduler-estimator\u0026#34;] ---\u0026gt; Using cache ---\u0026gt; af44d54da19a Successfully built af44d54da19a Successfully tagged swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-scheduler-estimator:latest make: Leaving directory \u0026#39;/root/Develop/gopath/src/karmada\u0026#39; Waiting for the host clusters to be ready... Waiting for kubeconfig file /root/.kube/karmada.config and clusters karmada-host to be ready... "}]